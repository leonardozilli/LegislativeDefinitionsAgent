{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection, MilvusClient\n",
    "import logging\n",
    "from typing import Any\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from milvus_model.hybrid import BGEM3EmbeddingFunction\n",
    "from LegalDefAgent.src.settings import settings\n",
    "from torch.cuda import is_available as cuda_available\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VectorDBBuilder:\n",
    "    def __init__(self):\n",
    "        self.config = settings\n",
    "        self.milvus_uri = self.config.MILVUSDB_URI\n",
    "        self.collection_name = self.config.MILVUSDB_COLLECTION_NAME\n",
    "        self.batch_size = self.config.DB_CONFIG.BATCH_SIZE\n",
    "        self.ef = BGEM3EmbeddingFunction(\n",
    "                model_name='BAAI/bge-m3',\n",
    "                device='cuda' if cuda_available() else 'cpu',\n",
    "                use_fp16=True if cuda_available() else False #set to false if device='cpu'\n",
    "            )\n",
    "        self.dense_dim = self.ef.dim[\"dense\"]\n",
    "        \n",
    "    def setup_collection(self) -> Collection:\n",
    "        \"\"\"Setup Milvus collection with proper schema.\"\"\"\n",
    "        fields = [\n",
    "            FieldSchema(\n",
    "                name=\"id\", \n",
    "                dtype=DataType.INT64,\n",
    "                is_primary=True, \n",
    "                #auto_id=True, \n",
    "                #max_length=100\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"definition_text\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=5000\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"definendum_label\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=256\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"dataset\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=10\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"document_id\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=40\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"frbr_work\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=120\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"frbr_expression\", \n",
    "                dtype=DataType.VARCHAR, \n",
    "                max_length=120\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"sparse_vector\", \n",
    "                dtype=DataType.SPARSE_FLOAT_VECTOR,\n",
    "            ),\n",
    "            FieldSchema(\n",
    "                name=\"dense_vector\", \n",
    "                dtype=DataType.FLOAT_VECTOR,\n",
    "                dim=self.dense_dim\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        schema = CollectionSchema(fields, \"Definitions embeddings\")\n",
    "        collection_name = \"Definitions\"\n",
    "        \n",
    "        # Drop existing collection if it exists\n",
    "        if utility.has_collection(collection_name):\n",
    "            Collection(collection_name).drop()\n",
    "            \n",
    "        collection = Collection(\n",
    "            collection_name, \n",
    "            schema, \n",
    "            consistency_level=\"Strong\"\n",
    "        )\n",
    "        \n",
    "        # Create and load index\n",
    "        sparse_index = {\n",
    "            \"index_type\": \"SPARSE_INVERTED_INDEX\", \n",
    "            \"metric_type\": \"IP\"\n",
    "        }\n",
    "        dense_index = {\n",
    "            \"index_type\": \"FLAT\", \n",
    "            \"metric_type\": \"COSINE\"\n",
    "        }\n",
    "        collection.create_index(\"sparse_vector\", sparse_index)\n",
    "        collection.create_index(\"dense_vector\", dense_index)\n",
    "        collection.load()\n",
    "        \n",
    "        return collection\n",
    "    \n",
    "    def build_vector_db(self, df: pl.DataFrame, defs_embeddings=None, definendum_embeddings=None) -> None:\n",
    "        \"\"\"Build vector database from processed definitions.\"\"\"\n",
    "        logger.info(\"Building vector database...\")\n",
    "        try:\n",
    "\n",
    "            Path(self.milvus_uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "            client = MilvusClient(\n",
    "                uri=self.milvus_uri\n",
    "            )\n",
    "            connections.connect(uri=self.milvus_uri)\n",
    "            \n",
    "            # Setup collection\n",
    "            collection = self.setup_collection()\n",
    "\n",
    "            definendum_list = (df.select(\n",
    "                pl.col('label')\n",
    "                .str.replace('#', '')\n",
    "                .str.replace(r'([a-zà-ÿ])([A-Z])', r'${1} ${2}', n=-1)  # Add space between lowercase and uppercase letters\n",
    "                .str.to_lowercase()  # Convert to lowercase after splitting\n",
    "            )\n",
    "            )['label'].to_list()\n",
    "\n",
    "            \n",
    "            # Generate embeddings and insert in batches\n",
    "            for i in range(0, len(df), self.batch_size):\n",
    "                batch_df = df.slice(i, self.batch_size)\n",
    "                \n",
    "                # Generate embeddings for the batch\n",
    "                batch_texts = batch_df['definition_text'].to_list()\n",
    "                if not defs_embeddings:\n",
    "                    batch_embeddings = self.ef(batch_texts)\n",
    "                else:\n",
    "                    batch_sparse_embeddings = definendum_embeddings['sparse'][i:i+self.batch_size]\n",
    "                    batch_dense_embeddings = defs_embeddings['dense'][i:i+self.batch_size]\n",
    "                \n",
    "                # Prepare batch data\n",
    "                batch_data = [\n",
    "                    batch_df['id'].to_list(),\n",
    "                    batch_df['definition_text'].to_list(),\n",
    "                    batch_df['label'].to_list(),\n",
    "                    batch_df['def_n'].to_list(),\n",
    "                    batch_df['dataset'].to_list(),\n",
    "                    batch_df['document_id'].to_list(),\n",
    "                    batch_df['frbr_work'].to_list(),\n",
    "                    batch_df['frbr_expression'].to_list(),\n",
    "                    batch_sparse_embeddings,\n",
    "                    batch_dense_embeddings\n",
    "                ]\n",
    "                \n",
    "                # Insert batch\n",
    "                collection.insert(batch_data)\n",
    "                \n",
    "            logger.info(f\"Inserted {collection.num_entities} entities into vector database\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building vector database: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            connections.disconnect(alias='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df = pl.read_parquet('../data/definitions_corpus/definitions.parquet')\n",
    "\n",
    "with open('../data/definendums_embeddings.pkl', 'rb') as f:\n",
    "    definendums_emb = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('../data/def_embeddings.pkl', 'rb') as f:\n",
    "    defs_emb = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44322e184bb24b94af505b7aa3b286f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder = VectorDBBuilder()\n",
    "\n",
    "builder.build_vector_db(df=df, defs_embeddings=defs_emb, definendum_embeddings=definendums_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class SupervisorState(TypedDict):\n",
    "    input: str\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "\n",
    "\n",
    "# Define subgraph\n",
    "class DefinitionAgentState(TypedDict):\n",
    "    question: str\n",
    "    query: Dict[str, str]\n",
    "    retrieved_definitions: list[str]\n",
    "    relevant_definitions: list[str]\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "\n",
    "def subgraph_node_1(state: DefinitionAgentState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "\n",
    "def subgraph_node_2(state: DefinitionAgentState):\n",
    "    # note that this node is using a state key ('bar') that is only available in the subgraph\n",
    "    # and is sending update on the shared state key ('foo')\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "\n",
    "subgraph_builder = StateGraph(DefinitionAgentState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class SupervisorState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "\n",
    "def node_1(state: SupervisorState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(SupervisorState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "# note that we're adding the compiled subgraph as a node to the parent graph\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_1': {'foo': 'hi! foo'}}\n",
      "{'node_2': {'foo': 'hi! foobar'}}\n",
      "\n",
      "((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:2a0eb3a6-537b-21d4-1e82-09d2163162bc',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:2a0eb3a6-537b-21d4-1e82-09d2163162bc',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"foo\": \"foo\"}):\n",
    "    print(chunk)\n",
    "\n",
    "print()\n",
    "for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from LegalDefAgent.src import utils\n",
    "from LegalDefAgent.src.settings import settings\n",
    "\n",
    "members = [\"researcher\", \"coder\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def tavily_tool(urls) -> str:\n",
    "    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n",
    "    return \"The meaning of life is 33.\"\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    llm, tools=[tavily_tool], state_modifier=\"You are a researcher. DO NOT do any math.\"\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "code_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "\n",
    "\n",
    "def code_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = code_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"coder\", code_node)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'supervisor': None})\n",
      "----\n",
      "(('researcher:0ca49cf6-2cd5-2dee-734a-586fed6e5337',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6v95', 'function': {'arguments': '{\"urls\":\"https://www.example.com\"}', 'name': 'tavily_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 959, 'total_tokens': 1047, 'completion_time': 0.16, 'prompt_time': 0.030585387, 'queue_time': 0.022670333, 'total_time': 0.190585387}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a89004bf-434e-4a53-b649-64f88a2d7ca5-0', tool_calls=[{'name': 'tavily_tool', 'args': {'urls': 'https://www.example.com'}, 'id': 'call_6v95', 'type': 'tool_call'}], usage_metadata={'input_tokens': 959, 'output_tokens': 88, 'total_tokens': 1047})]}})\n",
      "----\n",
      "(('researcher:0ca49cf6-2cd5-2dee-734a-586fed6e5337',), {'tools': {'messages': [ToolMessage(content='The meaning of life is 33.', name='tavily_tool', id='3a4ad9eb-ee04-4e54-8bbc-a791d137c1da', tool_call_id='call_6v95')]}})\n",
      "----\n",
      "(('researcher:0ca49cf6-2cd5-2dee-734a-586fed6e5337',), {'agent': {'messages': [AIMessage(content='The meaning of life is 33.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 1047, 'total_tokens': 1058, 'completion_time': 0.02, 'prompt_time': 0.033210905, 'queue_time': 0.023152581999999998, 'total_time': 0.053210905}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2a3ca350-0e8f-42f9-b736-45bc73164878-0', usage_metadata={'input_tokens': 1047, 'output_tokens': 11, 'total_tokens': 1058})]}})\n",
      "----\n",
      "((), {'researcher': {'messages': [HumanMessage(content='The meaning of life is 33.', additional_kwargs={}, response_metadata={}, name='researcher')]}})\n",
      "----\n",
      "((), {'supervisor': None})\n",
      "----\n",
      "(('researcher:1b25c9cf-4452-eeb5-3206-e4e94c6017e4',), {'agent': {'messages': [AIMessage(content='That is an interesting perspective!  What makes you think that?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 978, 'total_tokens': 994, 'completion_time': 0.029090909, 'prompt_time': 0.033278504, 'queue_time': 0.034824568, 'total_time': 0.062369413}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-59b68669-ff69-4e77-a6ae-abf022b23866-0', usage_metadata={'input_tokens': 978, 'output_tokens': 16, 'total_tokens': 994})]}})\n",
      "----\n",
      "((), {'researcher': {'messages': [HumanMessage(content='That is an interesting perspective!  What makes you think that?\\n', additional_kwargs={}, response_metadata={}, name='researcher')]}})\n",
      "----\n",
      "((), {'supervisor': None})\n",
      "----\n",
      "(('researcher:42bb3c52-856f-d395-6c5b-809f0b754d53',), {'agent': {'messages': [AIMessage(content=\"That was a joke!  There isn't really a single answer to the meaning of life, as it's a deeply personal and philosophical question.  What do you think the meaning of life is? \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1002, 'total_tokens': 1048, 'completion_time': 0.083636364, 'prompt_time': 0.032141129, 'queue_time': 0.021692446000000004, 'total_time': 0.115777493}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-92a50d4f-d49c-47e3-b53c-feb93417ac27-0', usage_metadata={'input_tokens': 1002, 'output_tokens': 46, 'total_tokens': 1048})]}})\n",
      "----\n",
      "((), {'researcher': {'messages': [HumanMessage(content=\"That was a joke!  There isn't really a single answer to the meaning of life, as it's a deeply personal and philosophical question.  What do you think the meaning of life is? \\n\", additional_kwargs={}, response_metadata={}, name='researcher')]}})\n",
      "----\n",
      "((), {'supervisor': None})\n",
      "----\n",
      "(('researcher:0591a606-c188-4f4a-0f95-30835d4bf9fa',), {'agent': {'messages': [AIMessage(content=\"That's a great point. It's definitely a question that each person has to answer for themselves.  \\n\\nI don't have personal beliefs or experiences to draw on, so I can't say what the meaning of life is for me.  But I can access and process information from the real world and I can tell you that people have been pondering this question for centuries. Some find meaning in their relationships, others in their work or hobbies, and others in their spiritual beliefs. \\n\\nWhat do YOU think gives life meaning?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 1056, 'total_tokens': 1171, 'completion_time': 0.209090909, 'prompt_time': 0.043251628, 'queue_time': 0.020399131, 'total_time': 0.252342537}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-98cafabc-3f63-4837-ba71-fdc526c70ab2-0', usage_metadata={'input_tokens': 1056, 'output_tokens': 115, 'total_tokens': 1171})]}})\n",
      "----\n",
      "((), {'researcher': {'messages': [HumanMessage(content=\"That's a great point. It's definitely a question that each person has to answer for themselves.  \\n\\nI don't have personal beliefs or experiences to draw on, so I can't say what the meaning of life is for me.  But I can access and process information from the real world and I can tell you that people have been pondering this question for centuries. Some find meaning in their relationships, others in their work or hobbies, and others in their spiritual beliefs. \\n\\nWhat do YOU think gives life meaning?  \\n\\n\", additional_kwargs={}, response_metadata={}, name='researcher')]}})\n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the meaning of life?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m----\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1649\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1656\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/pregel/runner.py:133\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    131\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/concurrent/futures/_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"What's the meaning of life?\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_agent = create_react_agent(\n",
    "    llm, tools=[tavily_tool], state_modifier=\"You are a researcher. DO NOT do any math.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, OpenWeatherMapQueryRun\n",
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnableSerializable\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.managed import RemainingSteps\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from LegalDefAgent.src.llm import get_model\n",
    "from LegalDefAgent.src.settings import settings\n",
    "from LegalDefAgent.src.retriever import vector_store, exist_db\n",
    "\n",
    "\n",
    "# ---- tools\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "vectorstore = vector_store.setup_vectorstore()\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "async def query_vectorstore(query: dict):\n",
    "    #implement hybrid search\n",
    "    retrieved_definitions = retriever.invoke(query)\n",
    "\n",
    "    return [{'definition_text': d.page_content, 'metadata': {'dataset': d.metadata['dataset'], 'document': d.metadata['document_id']}} for d in retrieved_definitions[:2]]\n",
    "\n",
    "\n",
    "async def definition_search(query: str, legislation: str = None, date_filters: tuple = None) -> str:\n",
    "    \"\"\"\n",
    "    Searches and retrieves the most similar definitions to the given query in a vector DB.\n",
    "\n",
    "    Args:\n",
    "        definendum: The term to be defined, as extracted from the user's query.\n",
    "        legislation: The legislation to search in.\n",
    "            possible values: \"EU\", \"IT\", None\n",
    "        date_filters: Date filters in the form of a tuple (from_date, to_date) to apply to the search.\n",
    "            e.g. (\"2021-01-01\", \"2021-12-31\")\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_definitions = await query_vectorstore(query)\n",
    "\n",
    "    return retrieved_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-mini': <OpenAIModelName.GPT_4O_MINI: 'gpt-4o-mini'>,\n",
       " 'gpt-4o': <OpenAIModelName.GPT_4O: 'gpt-4o'>,\n",
       " 'llama3-8b-8192': <GroqModelName.LLAMA_3_8B: 'groq-llama3-8b-8192'>,\n",
       " 'llama3-70b-8192': <GroqModelName.LLAMA_3_70B: 'groq-llama3-70b-8192'>,\n",
       " 'llama-3.3-70b-versatile': <GroqModelName.LLAMA_33_70B: 'groq-llama-3.3-70b-versatile'>,\n",
       " 'gemma2-9b-it': <GroqModelName.GEMMA2_9B_IT: 'groq-gemma2-9b-it'>,\n",
       " 'open-mistral-nemo': <MistralModelName.NEMO_12B: 'open-mistral-nemo'>,\n",
       " 'gemma2:2b': <OllamaModelName.GEMMA2_2B: 'ollama-gemma2:2b'>,\n",
       " 'llama3.2': <OllamaModelName.LLAMA_32_3B: 'ollama-llama3.2'>,\n",
       " 'phi3': <OllamaModelName.PHI3_4B: 'ollama-phi3'>,\n",
       " 'fake': <FakeModelName.FAKE: 'fake'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LegalDefAgent.src.llm import _MODEL_TABLE\n",
    "\n",
    "_MODEL_TABLE_INV = {v: k for k, v in _MODEL_TABLE.items()}\n",
    "_MODEL_TABLE_INV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'definition_text': \"examination: means a formalised test evaluating the person's knowledge and understanding;\",\n",
       "  'metadata': {'dataset': 'EurLex', 'document': '32015R0340.xml'}},\n",
       " {'definition_text': 'data item: means a single attribute of a complete data set, which is allocated a value that defines its current status;',\n",
       "  'metadata': {'dataset': 'EurLex', 'document': '32010R0073.xml'}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await definition_search(\"definendum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "from typing import Annotated, Literal, Sequence, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.callbacks import dispatch_custom_event\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "from LegalDefAgent.src.retriever import vector_store\n",
    "from LegalDefAgent.src.settings import settings\n",
    "from LegalDefAgent.src.schema.task_data import Task\n",
    "from LegalDefAgent.src.schema.definition import DefinitionsList, Definition, RelevantDefinitionsIDList\n",
    "from LegalDefAgent.src.schema.grader import DefinitionRelevance\n",
    "\n",
    "from LegalDefAgent.src.schema.task_data import Task\n",
    "import LegalDefAgent.src.utils as utils\n",
    "from LegalDefAgent.src.llm import get_model\n",
    "\n",
    "\n",
    "vectorstore = vector_store.setup_vectorstore()\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "\n",
    "async def query_vectorstore(query: str):\n",
    "    #implement hybrid search\n",
    "    retrieved_definitions = await retriever.ainvoke(query)\n",
    "\n",
    "    return utils.docs_list_to_json_list(retrieved_definitions)\n",
    "\n",
    "\n",
    "async def semantic_filter(model, question, retrieved_definitions):\n",
    "    parser = JsonOutputParser(pydantic_object=RelevantDefinitionsIDList)\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a legal expert assessing the relevance of legal definitions to a user question.\n",
    "        Below you are provided with a dcitionary containing definitions that were automatically retrieved.\n",
    "        Your task is to filter the list of definitions provided to you keeping only the relevant ones.\n",
    "        For each  the text of the definition contains keyword(s) or semantic meaning related to the user's question, keep it. Otherwise discard it.\n",
    "        Output only the relevant definitions using the formatting instructions provided.\n",
    "        VERY IMPORTANT NOTES:\n",
    "        * You can only answer with valid, directly parsable json.\n",
    "        * If you can't find any relevant definitions, you should output this: \"relevant_definitions\": []\\n\\n\n",
    "        Here are the formatting instructions: {format_instructions}\n",
    "        Here are the retrieved definitions: {context}\n",
    "        Here is the question asked by the user: {question}\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "    response = chain.invoke({\"context\": retrieved_definitions, \"question\": question}) # definendum or question??\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "async def definition_search(question: str, definendum: str, legislation: str | None = None, date_filters: tuple | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Searches and retrieves the most similar definitions to the given query in a vector DB.\n",
    "\n",
    "    Args:\n",
    "        question: The entire user's question.\n",
    "        definendum: The term to be defined, as extracted from the user's query.\n",
    "        legislation: The legislation to search in. Possible values: \"EU\", \"IT\", None\n",
    "        date_filters: Date filters in the form of a tuple (from_date, to_date) to apply to the search. e.g. (\"2021-01-01\", \"2021-12-31\")\n",
    "    \"\"\"\n",
    "\n",
    "    model = get_model(settings.DEFAULT_MODEL)\n",
    "\n",
    "    retrieved_definitions = await query_vectorstore(definendum)\n",
    "    print(retrieved_definitions)\n",
    "\n",
    "    relevant_definitions_ids = await semantic_filter(model, question, retrieved_definitions)\n",
    "    print(relevant_definitions_ids)\n",
    "\n",
    "    relevant_definitions = [d for d in retrieved_definitions if d['metadata']['id'] in relevant_definitions_ids['relevant_definitions']]\n",
    "\n",
    "\n",
    "    return relevant_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'metadata': {'dataset': 'EurLex', 'def_n': '#def_29', 'definendum_label': '#dog', 'document_id': '32020R0688.xml', 'frbr_expression': '/akn/eu/act/regulation/2019-12-17/688/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2019-12-17/688/!main', 'id': 8263}, 'definition_text': 'dog: means a kept animal of the Canis lupus species;'}, {'metadata': {'dataset': 'EurLex', 'def_n': '#def_10', 'definendum_label': '#dog', 'document_id': '32020R0689.xml', 'frbr_expression': '/akn/eu/act/regulation/2019-12-17/689/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2019-12-17/689/!main', 'id': 5008}, 'definition_text': 'dog: means a kept animal of the Canis lupus species;'}, {'metadata': {'dataset': 'EurLex', 'def_n': '#def_1', 'definendum_label': '#dog', 'document_id': '32019R2035.xml', 'frbr_expression': '/akn/eu/act/regulation/2019-06-28/2035/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2019-06-28/2035/!main', 'id': 2772}, 'definition_text': 'dog: means a kept animal of the Canis lupus species;'}, {'metadata': {'dataset': 'EurLex', 'def_n': '#def_9', 'definendum_label': '#documentaryCheck', 'document_id': '32013R0576.xml', 'frbr_expression': '/akn/eu/act/regulation/2013-06-12/576/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2013-06-12/576/!main', 'id': 4556}, 'definition_text': 'documentary check: means verification of the identification document accompanying the pet animal;'}, {'metadata': {'dataset': 'EurLex', 'def_n': '#def_1', 'definendum_label': '#animals', 'document_id': '32016R0429.xml', 'frbr_expression': '/akn/eu/act/regulation/2016-03-09/429/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2016-03-09/429/!main', 'id': 1786}, 'definition_text': 'animals: means vertebrate and invertebrate animals;'}, {'metadata': {'dataset': 'PDL', 'def_n': '#def_3', 'definendum_label': '#animaleDaPelliccia', 'document_id': '18PDL0002590_PD.xml', 'frbr_expression': '/akn/it/bill/propostaDiLegge/2018-03-23/177/ita@/!main', 'frbr_work': '/akn/it/bill/propostaDiLegge/2018-03-23/177/!main', 'id': 16498}, 'definition_text': 'animale da pelliccia: le seguenti specie di animali: cane procione (Nyctereutes procyonoides), capra della Mongolia (Ovis Steatopyga), castorino (detto nutria - Myocastor coypus), castoro (Castor canadensis), cincillà (Chinchilla laniger), coniglio (detto lapin - Oryctolagus cuniculus), coyote (Canis latrans), donnola (Mustela nivalis), ermellino (Mustela erminea), foca (Phocidae), gatto leopardo (Prionailurus bengalensis), karakul (detto astrakhan o agnello persiano - Ovis aries platyura), lince (Lynx), lontra (Lutra canadensis), marmotta (Marmota marmota), martora (Martes martes), moffetta (detta skunk - Mephitis mephitis), ocelot (Felis pardalis), ondatra (detto topo muschiato - Ondatra zybethica), opossum (Didelphis marsupialis), procione (Procyon lotor), puzzola (Mustela putorius), scoiattolo (Sciurus carolinensis), tasso (Meles meles), visone (Mustela vison o Neovison vison), volpe (Vulpes vulpes), zibellino (Martes zibellina), coccodrillo (Crocodylia), pitone (Python) e varano (Varanus);'}, {'metadata': {'dataset': 'EurLex', 'def_n': '#def_2', 'definendum_label': '#petAnimal', 'document_id': '32013R0576.xml', 'frbr_expression': '/akn/eu/act/regulation/2013-06-12/576/eng@/!main', 'frbr_work': '/akn/eu/act/regulation/2013-06-12/576/!main', 'id': 4549}, 'definition_text': 'pet animal: means an animal of a species listed in Annex I accompanying its owner or an authorised person during non-commercial movement, and which remains for the duration of such non-commercial movement under the responsibility of the owner or the authorised person;'}]\n",
      "{'relevant_definitions': [8263, 5008, 2772]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'dataset': 'EurLex',\n",
       "   'def_n': '#def_29',\n",
       "   'definendum_label': '#dog',\n",
       "   'document_id': '32020R0688.xml',\n",
       "   'frbr_expression': '/akn/eu/act/regulation/2019-12-17/688/eng@/!main',\n",
       "   'frbr_work': '/akn/eu/act/regulation/2019-12-17/688/!main',\n",
       "   'id': 8263},\n",
       "  'definition_text': 'dog: means a kept animal of the Canis lupus species;'},\n",
       " {'metadata': {'dataset': 'EurLex',\n",
       "   'def_n': '#def_10',\n",
       "   'definendum_label': '#dog',\n",
       "   'document_id': '32020R0689.xml',\n",
       "   'frbr_expression': '/akn/eu/act/regulation/2019-12-17/689/eng@/!main',\n",
       "   'frbr_work': '/akn/eu/act/regulation/2019-12-17/689/!main',\n",
       "   'id': 5008},\n",
       "  'definition_text': 'dog: means a kept animal of the Canis lupus species;'},\n",
       " {'metadata': {'dataset': 'EurLex',\n",
       "   'def_n': '#def_1',\n",
       "   'definendum_label': '#dog',\n",
       "   'document_id': '32019R2035.xml',\n",
       "   'frbr_expression': '/akn/eu/act/regulation/2019-06-28/2035/eng@/!main',\n",
       "   'frbr_work': '/akn/eu/act/regulation/2019-06-28/2035/!main',\n",
       "   'id': 2772},\n",
       "  'definition_text': 'dog: means a kept animal of the Canis lupus species;'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What's the definition of dog in the EurLex dataset?\"\n",
    "definendum = \"dog\"\n",
    "\n",
    "await definition_search(question=question, definendum=definendum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Message dict must contain 'role' and 'content' keys, got {'foo': 'bar'}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:319\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     msg_type \u001b[38;5;241m=\u001b[39m \u001b[43mmsg_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'role'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:321\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     msg_type \u001b[38;5;241m=\u001b[39m \u001b[43mmsg_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# None msg content is not allowed\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m      3\u001b[0m state \u001b[38;5;241m=\u001b[39m{   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m   ]\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrim_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:381\u001b[0m, in \u001b[0;36m_runnable_support.<locals>.wrapped\u001b[0;34m(messages, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableLambda(partial(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:825\u001b[0m, in \u001b[0;36mtrim_messages\u001b[0;34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_system \u001b[38;5;129;01mand\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m--> 825\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(token_counter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    827\u001b[0m     list_token_counter \u001b[38;5;241m=\u001b[39m token_counter\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:357\u001b[0m, in \u001b[0;36mconvert_to_messages\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/messages/utils.py:329\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    325\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage dict must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         msg \u001b[38;5;241m=\u001b[39m create_message(\n\u001b[1;32m    327\u001b[0m             message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mMESSAGE_COERCION_FAILURE\n\u001b[1;32m    328\u001b[0m         )\n\u001b[0;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     _message \u001b[38;5;241m=\u001b[39m _create_message_from_message_type(\n\u001b[1;32m    331\u001b[0m         msg_type, msg_content, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmsg_kwargs\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Message dict must contain 'role' and 'content' keys, got {'foo': 'bar'}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage, trim_messages\n",
    "\n",
    "state ={   \"messages\": [\n",
    "    {\n",
    "      \"content\": \"hi\",\n",
    "      \"additional_kwargs\": {},\n",
    "      \"response_metadata\": {},\n",
    "      \"type\": \"human\",\n",
    "      \"id\": \"fe160588-b249-40f3-92de-0e3887eb6025\",\n",
    "      \"example\": False\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"What's the definition of dog in the EurLex dataset?\",\n",
    "        \"additional_kwargs\": {},\n",
    "        \"response_metadata\": {},\n",
    "        \"type\": \"human\",\n",
    "        \"id\": \"b6f1d8f4-4e8b-4b7c-9b2f-5a1c4b1f4d6b\",\n",
    "        \"example\": False\n",
    "    },\n",
    "    {'foo': 'bar'}\n",
    "  ]\n",
    "}\n",
    "\n",
    "print(state['messages'][-1])\n",
    "state['messages'] = trim_messages(state['messages'], token_counter=len, strategy=\"last\", max_tokens=1)\n",
    "print(state['messages'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
