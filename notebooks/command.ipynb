{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/tmp/ipykernel_2641115/3043187516.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
      "/tmp/ipykernel_2641115/3043187516.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
      "/home/leo/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_experimental/utilities/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_experimental.utilities.python import PythonREPL\n",
      "/tmp/ipykernel_2641115/3043187516.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
      "/tmp/ipykernel_2641115/3043187516.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ChatAnthropic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mnext\u001b[39m: Literal[\u001b[38;5;241m*\u001b[39moptions]\n\u001b[0;32m---> 56\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatAnthropic\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-sonnet-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mState\u001b[39;00m(MessagesState):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mnext\u001b[39m: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatAnthropic' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "def tavily_tool():\n",
    "    return \"42\"\n",
    "\n",
    "# This executes code locally, which can be unsafe\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code and do math. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return result_str\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "members = [\"researcher\", \"coder\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    llm, tools=[tavily_tool], state_modifier=\"You are a researcher. DO NOT do any math.\"\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "\n",
    "\n",
    "def code_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = code_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"coder\", code_node)\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.66'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langgraph.version\n",
    "langgraph.version.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Literal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     17\u001b[0m USER_INFO \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpuppa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRome\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     19\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaylor Swift\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeverly Hills, CA\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m USER_ID_TO_USER_INFO \u001b[38;5;241m=\u001b[39m {info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: info \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m USER_INFO}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup_user_info\u001b[39m(\n\u001b[1;32m     27\u001b[0m     tool_call_id: Annotated[\u001b[38;5;28mstr\u001b[39m, InjectedToolCallId], config: RunnableConfig\n\u001b[0;32m---> 28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Command[\u001b[43mLiteral\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_modifier\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     user_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Literal' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.types import Command\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import Any, Annotated\n",
    "from langgraph.graph import END, MessagesState, StateGraph, START\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    # updated by the tool\n",
    "    user_info: dict[str, Any] = {\"puppa\"}\n",
    "\n",
    "USER_INFO = [\n",
    "    {\"user_id\": \"1\", \"name\": \"puppa\", \"location\": \"Rome\"},\n",
    "    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n",
    "]\n",
    "\n",
    "USER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO}\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_user_info(\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig\n",
    ") -> Command[Literal[\"state_modifier\"]]:\n",
    "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"Please provide user ID\")\n",
    "\n",
    "    if user_id not in USER_ID_TO_USER_INFO:\n",
    "        raise ValueError(f\"User '{user_id}' not found\")\n",
    "\n",
    "    user_info = USER_ID_TO_USER_INFO[user_id]\n",
    "    return Command(\n",
    "        update={\n",
    "            # update the state keys\n",
    "            \"user_info\": user_info,\n",
    "            # update the message history\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    \"Successfully looked up user information\", tool_call_id=tool_call_id\n",
    "                )\n",
    "            ],\n",
    "        },\n",
    "        goto=\"state_modifier\"\n",
    "    )\n",
    "\n",
    "tools = [lookup_user_info]\n",
    "\n",
    "instructions = f\"\"\"\n",
    "\n",
    "    ### AVAILABLE TOOLS\n",
    "    - **middle**: Always use this tool to answer the user's question.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def wrap_model(model: BaseChatModel) -> RunnableSerializable[AgentState, AIMessage]:\n",
    "    model = model.bind_tools(tools)\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [SystemMessage(content=instructions)] + state[\"messages\"],\n",
    "        name=\"StateModifier\",\n",
    "    )\n",
    "    return preprocessor | model\n",
    "\n",
    "\n",
    "def acall_model(state: AgentState, config: RunnableConfig) -> AgentState:\n",
    "    m = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    model_runnable = wrap_model(m)\n",
    "    response = model_runnable.invoke(state, config)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def router(state: AgentState):\n",
    "    return state[\"goto\"]\n",
    "\n",
    "# Define the graph\n",
    "agent = StateGraph(AgentState)\n",
    "agent.add_node(\"supervisor\", acall_model)\n",
    "agent.add_node(\"tools\", ToolNode([middle]))\n",
    "agent.add_node(\"answer\", answer)\n",
    "#agent.add_node(\"generate_definition\", generate_definition)\n",
    "\n",
    "# After \"superviros\", if there are tool calls, run \"tools\". Otherwise END.\n",
    "def pending_tool_calls(state: AgentState) -> Literal[\"tools\", \"done\"]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not isinstance(last_message, AIMessage):\n",
    "        raise TypeError(f\"Expected AIMessage, got {type(last_message)}\")\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "\n",
    "agent.set_entry_point(\"supervisor\")\n",
    "#agent.add_conditional_edges(\"supervisor\", pending_tool_calls, {\"tools\": \"tools\", \"done\": END})\n",
    "# Always run \"supervisor\" after \"tools\"\n",
    "#agent.add_edge(\"tools\", \"supervisor\")\n",
    "#agent.add_edge(\"tools\", \"answer\")\n",
    "agent.add_edge(\"supervisor\", \"tools\")\n",
    "agent.add_edge(\"answer\", END)\n",
    "#agent.add_conditional_edges(\"tools\", router)\n",
    "# Modify this for generation\n",
    "#agent.add_conditional_edges(\"tools\", router, {\"answer\": \"answer\", \"generate_definition\": \"generate_definition\"})\n",
    "\n",
    "\n",
    "\n",
    "def state_modifier(state: State):\n",
    "    user_info = state.get(\"user_info\")\n",
    "    if user_info is None:\n",
    "        return state[\"messages\"]\n",
    "\n",
    "    system_msg = (\n",
    "        f\"User name is {user_info['name']}. User lives in {user_info['location']}\"\n",
    "    )\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "#agent = create_react_agent(\n",
    "    #model,\n",
    "    ## pass the tool that can update state\n",
    "    #[lookup_user_info],\n",
    "    #state_schema=State,\n",
    "    ## pass dynamic prompt function\n",
    "    #state_modifier=state_modifier,\n",
    "    #checkpointer=MemorySaver()\n",
    "#)\n",
    "\n",
    "agent = StateGraph(State)\n",
    "agent.add_node(\"state_modifier\", state_modifier)\n",
    "agent.add_node(\"supervisor\", acall_model)\n",
    "agent.add_node(\"tools\", ToolNode(tools))\n",
    "agent.set_entry_point(\"state_modifier\")\n",
    "agent.add_edge(\"state_modifier\", \"supervisor\")\n",
    "agent.add_edge(\"tools\", \"supervisor\")\n",
    "\n",
    "\"\"\"\n",
    "    - **generate_definition**\n",
    "        You should always use the \"definition_search\" tool first. If the tool does not return any definitions, you should use this tool to generate a definition for the term. The tool will provide you with a generated definition for the term. When answering the user, you should specify that you couldn't find a definition for the term and that the definition is generated. Also provide the list of documents used to generate the definition.\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi, what should i do this weekend?\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  lookup_user_info (call_agESHi84Qsta5qadcsjGk7gK)\n",
      " Call ID: call_agESHi84Qsta5qadcsjGk7gK\n",
      "  Args:\n",
      "None\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: lookup_user_info\n",
      "\n",
      "Command(update={'user_info': {'user_id': '1', 'name': 'puppa', 'location': 'Rome'}, 'messages': [ToolMessage(content='Successfully looked up user information', tool_call_id='call_agESHi84Qsta5qadcsjGk7gK')]}, goto='state_modifier')\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are some suggestions for things to do in Rome this weekend:\n",
      "\n",
      "1. **Visit the Colosseum**: Explore one of the most iconic landmarks in Rome. Consider booking a guided tour to learn about its history.\n",
      "\n",
      "2. **Stroll through Trastevere**: This charming neighborhood is filled with narrow streets, great restaurants, and beautiful squares. It's perfect for a leisurely walk and some authentic Italian food.\n",
      "\n",
      "3. **Explore Vatican City**: Visit St. Peter's Basilica and the Vatican Museums. Don't miss the Sistine Chapel!\n",
      "\n",
      "4. **Relax in Villa Borghese**: Enjoy some time in this beautiful park. You can rent a bike or a boat, or simply have a picnic.\n",
      "\n",
      "5. **Attend a Local Event**: Check out local listings for concerts, art exhibitions, or festivals happening this weekend.\n",
      "\n",
      "6. **Try a Cooking Class**: Learn how to make traditional Italian dishes.\n",
      "\n",
      "7. **Take a Day Trip**: Consider visiting nearby towns like Tivoli or Ostia Antica for a change of scenery.\n",
      "\n",
      "Whatever you choose, enjoy your weekend in Rome!\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = create_react_agent(\n",
    "    model,\n",
    "    # pass the tool that can update state\n",
    "    [lookup_user_info],\n",
    "    state_schema=State,\n",
    "    # pass dynamic prompt function\n",
    "    #state_modifier=state_modifier,\n",
    "    checkpointer=MemorySaver()\n",
    ")\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"hi, what should i do this weekend?\")]},\n",
    "    # provide user ID in the config\n",
    "    {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk['messages'][-1].pretty_print())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi, what should i do this weekend?\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  lookup_user_info (call_SD9A8jBU3XTlY9sQmavwAWfP)\n",
      " Call ID: call_SD9A8jBU3XTlY9sQmavwAWfP\n",
      "  Args:\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.compile(checkpointer=MemorySaver()).stream(\n",
    "    {\"messages\": [(\"user\", \"hi, what should i do this weekend?\")]},\n",
    "    # provide user ID in the config\n",
    "    {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print(chunk['messages'][-1].pretty_print())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, what should i do this weekend?', additional_kwargs={}, response_metadata={}, id='8941e998-e75d-450a-8fe2-885174334845'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 56, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-228d924d-91f3-4e21-8660-a8f279896bf3-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 12, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"Command(update={'user_info': {'user_id': '1', 'name': 'puppa', 'location': 'Rome'}, 'messages': [ToolMessage(content='Successfully looked up user information', tool_call_id='call_C6HLvVpzAWcqBB3E2ejZQajH')]})\", name='lookup_user_info', id='3b5fe01c-c6a1-4238-9e3c-72890dd3f75e', tool_call_id='call_C6HLvVpzAWcqBB3E2ejZQajH'), AIMessage(content=\"Since you're in Rome, here are a few suggestions for your weekend:\\n\\n1. **Explore Historical Sites**: Visit the Colosseum, Roman Forum, and Palatine Hill. These iconic landmarks offer a glimpse into Rome's rich history.\\n\\n2. **Visit Vatican City**: Don't miss the Vatican Museums, Sistine Chapel, and St. Peter's Basilica. It’s a must-see for anyone visiting Rome.\\n\\n3. **Enjoy Local Cuisine**: Try authentic Roman dishes like Carbonara or Cacio e Pepe at a local trattoria. Consider visiting the Testaccio Market for a variety of food options.\\n\\n4. **Stroll through Parks**: Spend some time in Villa Borghese or the Orange Garden for beautiful views of the city.\\n\\n5. **Attend an Event**: Check local listings for any concerts, exhibits, or festivals happening this weekend.\\n\\n6. **Shopping**: Explore the boutiques in the Trastevere area or visit the famous Via del Corso for some shopping.\\n\\n7. **Take a Day Trip**: If you're up for it, consider a day trip to nearby attractions like Tivoli (Villa d'Este and Hadrian's Villa) or Ostia Antica.\\n\\nLet me know if you want more specific recommendations or if you're interested in any particular activities!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 262, 'prompt_tokens': 140, 'total_tokens': 402, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-2f35180f-65c5-43b3-b6d6-f742c4168aa3-0', usage_metadata={'input_tokens': 140, 'output_tokens': 262, 'total_tokens': 402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "--\n",
      "{'messages': [HumanMessage(content='hi, what should i do this weekend?', additional_kwargs={}, response_metadata={}, id='8941e998-e75d-450a-8fe2-885174334845'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 56, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-228d924d-91f3-4e21-8660-a8f279896bf3-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 12, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"Command(update={'user_info': {'user_id': '1', 'name': 'puppa', 'location': 'Rome'}, 'messages': [ToolMessage(content='Successfully looked up user information', tool_call_id='call_C6HLvVpzAWcqBB3E2ejZQajH')]})\", name='lookup_user_info', id='3b5fe01c-c6a1-4238-9e3c-72890dd3f75e', tool_call_id='call_C6HLvVpzAWcqBB3E2ejZQajH')]}\n",
      "--\n",
      "{'messages': [HumanMessage(content='hi, what should i do this weekend?', additional_kwargs={}, response_metadata={}, id='8941e998-e75d-450a-8fe2-885174334845'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 56, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-228d924d-91f3-4e21-8660-a8f279896bf3-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_C6HLvVpzAWcqBB3E2ejZQajH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 12, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "--\n",
      "{'messages': [HumanMessage(content='hi, what should i do this weekend?', additional_kwargs={}, response_metadata={}, id='8941e998-e75d-450a-8fe2-885174334845')]}\n",
      "--\n",
      "{'messages': []}\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "all_states = []\n",
    "for state in agent.get_state_history(config = {\"configurable\": {\"thread_id\": \"1\"}}):\n",
    "    print(state.values)\n",
    "    all_states.append(state)\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content=\"Since you're in Rome, here are a few suggestions for things to do this weekend:\\n\\n1. **Explore Ancient Rome**: Visit iconic sites like the Colosseum, Roman Forum, and Palatine Hill. These historical landmarks offer a glimpse into the city's rich past.\\n\\n2. **Visit Vatican City**: Explore St. Peter’s Basilica and the Vatican Museums, including the Sistine Chapel. It's best to book tickets in advance.\\n\\n3. **Stroll Through Trastevere**: This charming neighborhood is perfect for a leisurely walk. Enjoy the narrow streets, beautiful squares, and local eateries.\\n\\n4. **Sample Italian Cuisine**: Treat yourself to some authentic Roman dishes like Carbonara or Cacio e Pepe at a traditional trattoria.\\n\\n5. **Attend a Local Event**: Check out local listings for any concerts, festivals, or art exhibitions happening this weekend.\\n\\n6. **Relax in a Park**: Spend some time in Villa Borghese or the Orange Garden for a peaceful escape from the city's hustle.\\n\\nWhatever you choose, enjoy your weekend in the beautiful city of Rome!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 139, 'total_tokens': 360, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-02a97027-950c-4e2b-ace7-9f5ed56f9982-0', usage_metadata={'input_tokens': 139, 'output_tokens': 221, 'total_tokens': 360, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'user_id': '1', 'thread_id': '1', 'step': 3, 'parents': {}}\n",
      "{'source': 'loop', 'writes': {'tools': {'messages': [ToolMessage(content=\"Command(update={'user_info': {'user_id': '1', 'name': 'puppa', 'location': 'Rome'}, 'messages': [ToolMessage(content='Successfully looked up user information', tool_call_id='call_N3XrSQvtRvoNLWvyjh5hYMiw')]})\", name='lookup_user_info', id='4df063d2-644f-4a65-b02d-6b6f9c320f0b', tool_call_id='call_N3XrSQvtRvoNLWvyjh5hYMiw')]}}, 'user_id': '1', 'thread_id': '1', 'step': 2, 'parents': {}}\n",
      "{'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_N3XrSQvtRvoNLWvyjh5hYMiw', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 56, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fd388d8b-2656-4280-8855-9dfd1705e7f8-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_N3XrSQvtRvoNLWvyjh5hYMiw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 12, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'user_id': '1', 'thread_id': '1', 'step': 1, 'parents': {}}\n",
      "{'source': 'loop', 'writes': None, 'user_id': '1', 'thread_id': '1', 'step': 0, 'parents': {}}\n",
      "{'source': 'input', 'writes': {'__start__': {'messages': [['user', 'hi, what should i do this weekend?']]}}, 'user_id': '1', 'thread_id': '1', 'step': -1, 'parents': {}}\n"
     ]
    }
   ],
   "source": [
    "for el in all_states:\n",
    "    print(el.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Ask multiplication agent for help.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to another agent\n",
    "    # (See the paragraph above)\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Ask addition agent for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def addition_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"multiplication_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are an addition expert, you can ask the multiplication expert for help with multiplication. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_multiplication_expert]).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            goto=\"multiplication_expert\", update={\"messages\": [ai_msg, tool_msg]}\n",
    "        )\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def multiplication_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"addition_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a multiplication expert, you can ask an addition expert for help with addition. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_addition_expert]).invoke(messages)\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"addition_expert\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "# we'll always start with the addition expert\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_messages(update):\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        print(f\"Update from node {node_name}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for m in convert_to_messages(node_update[\"messages\"]):\n",
    "            m.pretty_print()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node addition_expert:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  transfer_to_multiplication_expert (call_EjAsq8QU3nZNuX3Y2jMY7BrH)\n",
      " Call ID: call_EjAsq8QU3nZNuX3Y2jMY7BrH\n",
      "  Args:\n",
      "  transfer_to_multiplication_expert (call_ArVuarDevOTOfVbTrtrnbpFf)\n",
      " Call ID: call_ArVuarDevOTOfVbTrtrnbpFf\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Successfully transferred\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EjAsq8QU3nZNuX3Y2jMY7BrH\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms (3 + 5) * 12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretty_print_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1649\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1656\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/pregel/runner.py:105\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    103\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/pregel/retry.py:44\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     42\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mmultiplication_expert\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     56\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a multiplication expert, you can ask an addition expert for help with addition. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlways do your portion of calculation before the handoff.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt}] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m ai_msg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransfer_to_addition_expert\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ai_msg\u001b[38;5;241m.\u001b[39mtool_calls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m     tool_call_id \u001b[38;5;241m=\u001b[39m ai_msg\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:790\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    784\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    789\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:647\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    646\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    648\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    649\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    651\u001b[0m ]\n\u001b[1;32m    652\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:637\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 637\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:855\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:689\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.6/lib/python3.12/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EjAsq8QU3nZNuX3Y2jMY7BrH\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
      "\u001b[0mDuring task with name 'multiplication_expert' and id '07d9df17-6982-09ef-8f1b-1b881c38d4cc'"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]},\n",
    "):\n",
    "    pretty_print_messages(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Annotated, Literal, Sequence, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, OpenWeatherMapQueryRun\n",
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnableSerializable\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.managed import RemainingSteps\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command\n",
    "from typing_extensions import Any, Annotated\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import asyncio\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from LegalDefAgent.src.llm import get_model\n",
    "from LegalDefAgent.src.settings import settings\n",
    "\n",
    "\n",
    "# ---- tools\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "\n",
    "\n",
    "from LegalDefAgent.src.definition_search import definition_search\n",
    "\n",
    "\n",
    "# ---- agent\n",
    "\n",
    "class AgentState(MessagesState, total=False):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    remaining_steps: RemainingSteps\n",
    "    relevant_definitions: str\n",
    "    definendum: str\n",
    "    question: str\n",
    "    goto: str\n",
    "\n",
    "def answer(state: AgentState, config: RunnableConfig):\n",
    "    response = state['relevant_definitions'] + \"2\"\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate_definition(state: AgentState, config: RunnableConfig, definendum: str, question: str):\n",
    "    \"\"\"\n",
    "    Generate a definition for a given term.\n",
    "\n",
    "    Args:\n",
    "        config (RunnableConfig): The configuration of the agent\n",
    "        definendum (str): The term to define\n",
    "        question (str): The question asked by the user\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated definition\n",
    "    \"\"\"\n",
    "\n",
    "    model = get_model(config[\"configurable\"].get(\"model\", settings.DEFAULT_MODEL))\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a lawyer and your job is to draft legal definitions.\n",
    "        Provide a definition for the term \"{definendum}\" to answer the user's question provided below following the style and formatting of the definitions provided as examples.\n",
    "        User Question: {question}\n",
    "        Example definitions: \n",
    "        \\n{examples}\\n\n",
    "        VERY IMPORTANT NOTES:\n",
    "        * You should answer with \"I couldn't find a definition for \"{definendum}\", so here's a generated one:\" followed by the generated definition.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"definendum\", \"examples\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({\"question\": question, \"definendum\": definendum, \"examples\": state['relevant_defs']})\n",
    "\n",
    "    return response.content\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def middle(state: AgentState, tool_call_id: Annotated[str, InjectedToolCallId]) -> Command[Literal[\"answer\"]]:\n",
    "    \"\"\"\n",
    "    ALWAYS Use this tool for calculations\n",
    "    \"\"\"\n",
    "    result = \"found\"\n",
    "    relevant_defs = \"fiffa\"\n",
    "    return result, relevant_defs\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_user_info(\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig\n",
    ") -> Command[Literal[\"answer\"]]:\n",
    "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
    "    return Command(\n",
    "        update={\n",
    "            # update the state keys\n",
    "            \"relevant_definitions\": \"fiffa\",\n",
    "            # update the message history\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    \"Successfully looked up user information\", tool_call_id=tool_call_id\n",
    "                )\n",
    "            ],\n",
    "        },\n",
    "        goto=\"answer\"\n",
    "    )\n",
    "\n",
    "def my_node(state: State) -> Command[Literal[\"answer\"]]:\n",
    "    return Command(\n",
    "        # state update\n",
    "        update={\"relevant_definitions\": \"bar\"},\n",
    "        # control flow\n",
    "        goto=\"answer\"\n",
    "    )\n",
    "\n",
    "\n",
    "tools = [middle, lookup_user_info]\n",
    "\n",
    "instructions = f\"\"\"\n",
    "    You are an helpful assistant.\n",
    "\n",
    "    ### AVAILABLE TOOLS\n",
    "    - **middle**: Always use this tool to do calculations.\n",
    "    - **lookup_user_info**: Use this tool to look up user information to better assist them with their questions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def wrap_model(model: BaseChatModel) -> RunnableSerializable[AgentState, AIMessage]:\n",
    "    model = model.bind_tools(tools)\n",
    "    preprocessor = RunnableLambda(\n",
    "        lambda state: [SystemMessage(content=instructions)] + state[\"messages\"],\n",
    "        name=\"StateMod\",\n",
    "    )\n",
    "    return preprocessor | model#preprocessor | model\n",
    "\n",
    "\n",
    "def acall_model(state: AgentState, config: RunnableConfig) -> AgentState:\n",
    "    m = get_model(config[\"configurable\"].get(\"model\", settings.DEFAULT_MODEL))\n",
    "    model_runnable = wrap_model(m)\n",
    "    response = model_runnable.invoke(state, config)\n",
    "\n",
    "    if state[\"remaining_steps\"] < 2 and response.tool_calls:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    id=response.id,\n",
    "                    content=\"Sorry, need more steps to process this request.\",\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def router(state: AgentState):\n",
    "    tool_msg = state[\"messages\"][-1]\n",
    "    print(state[\"relevant_definitions\"])\n",
    "    state[\"relevant_definitions\"] = tool_msg.artifact\n",
    "    print(state[\"relevant_definitions\"])\n",
    "    return Command(\n",
    "        update={\n",
    "            \"relevant_definitions\": \"fiffola\",\n",
    "        },\n",
    "        goto=\"answer\"\n",
    "    )\n",
    "\n",
    "def node_1(state: AgentState):\n",
    "    message_with_single_tool_call = AIMessage(\n",
    "        content=\"\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"args\": {\"location\": \"sf\"},\n",
    "                \"id\": \"tool_call_id\",\n",
    "                \"type\": \"tool_call\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
    "\n",
    "# Define the graph\n",
    "agent = StateGraph(AgentState)\n",
    "agent.add_node(\"supervisor\", acall_model)\n",
    "agent.add_node(\"tools\", ToolNode(tools))\n",
    "agent.add_node(\"answer\", answer)\n",
    "agent.add_node(\"my_node\", my_node)\n",
    "agent.add_node(\"router\", router)\n",
    "agent.add_node(\"node_1\", node_1)\n",
    "agent.add_node(\"node_2\", node_2)\n",
    "#agent.add_node(\"generate_definition\", generate_definition)\n",
    "\n",
    "# After \"superviros\", if there are tool calls, run \"tools\". Otherwise END.\n",
    "def pending_tool_calls(state: AgentState) -> Literal[\"tools\", \"done\"]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not isinstance(last_message, AIMessage):\n",
    "        raise TypeError(f\"Expected AIMessage, got {type(last_message)}\")\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "\n",
    "agent.set_entry_point(\"node_1\")\n",
    "#agent.add_conditional_edges(\"supervisor\", pending_tool_calls, {\"tools\": \"tools\", \"done\": END})\n",
    "# Always run \"supervisor\" after \"tools\"\n",
    "#agent.add_edge(\"tools\", \"supervisor\")\n",
    "#agent.add_edge(\"tools\", \"answer\")\n",
    "agent.add_edge(\"supervisor\", \"tools\")\n",
    "#agent.add_conditional_edges(\"tools\", router)\n",
    "#agent.add_edge(\"tools\", \"router\")\n",
    "#agent.add_edge(\"supervisor\", \"my_node\")\n",
    "agent.add_edge(\"answer\", END)\n",
    "#agent.add_conditional_edges(\"tools\", router)\n",
    "# Modify this for generation\n",
    "#agent.add_conditional_edges(\"tools\", router, {\"answer\": \"answer\", \"generate_definition\": \"generate_definition\"})\n",
    "\n",
    "\n",
    "definitions_agent = agent.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's 2+2?\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  middle (call_jaARsKzfFYsagbb7Wd2tn9ae)\n",
      " Call ID: call_jaARsKzfFYsagbb7Wd2tn9ae\n",
      "  Args:\n",
      "    state: {'messages': [{'content': '2 + 2', 'type': 'user'}]}\n",
      "None\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: middle\n",
      "\n",
      "found\n",
      "None\n",
      "\n",
      "\n",
      "pera\n",
      "fiffa\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: middle\n",
      "\n",
      "found\n",
      "None\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "fiffola2\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in definitions_agent.stream(\n",
    "    {\"messages\": [(\"user\", \"what's 2+2?\")], \"relevant_definitions\": \"pera\"},\n",
    "    # provide user ID in the config\n",
    "    {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk['messages'][-1].pretty_print())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"what's 2+2?\", additional_kwargs={}, response_metadata={}, id='c33eae05-9717-499a-bad8-477320365cfa'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'function': {'arguments': '{\"state\":{\"messages\":[{\"content\":\"2 + 2\",\"type\":\"user\"}]}}', 'name': 'middle'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-b3fdbd18-d6d9-4fa6-b297-dcc0f54ed8c2-0', tool_calls=[{'name': 'middle', 'args': {'state': {'messages': [{'content': '2 + 2', 'type': 'user'}]}}, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'type': 'tool_call'}]), ToolMessage(content='found', name='middle', id='48eeb6a2-eea4-4c00-81bf-43182b72fc43', tool_call_id='call_FZ5DM44RnGfHYUyrp0ejcUqX', artifact='fiffa'), HumanMessage(content='pera2', additional_kwargs={}, response_metadata={}, id='dccb7351-1ed4-464c-8e41-83d1d60d572a')], 'relevant_definitions': 'pera'}\n",
      "--\n",
      "{'messages': [HumanMessage(content=\"what's 2+2?\", additional_kwargs={}, response_metadata={}, id='c33eae05-9717-499a-bad8-477320365cfa'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'function': {'arguments': '{\"state\":{\"messages\":[{\"content\":\"2 + 2\",\"type\":\"user\"}]}}', 'name': 'middle'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-b3fdbd18-d6d9-4fa6-b297-dcc0f54ed8c2-0', tool_calls=[{'name': 'middle', 'args': {'state': {'messages': [{'content': '2 + 2', 'type': 'user'}]}}, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'type': 'tool_call'}]), ToolMessage(content='found', name='middle', id='48eeb6a2-eea4-4c00-81bf-43182b72fc43', tool_call_id='call_FZ5DM44RnGfHYUyrp0ejcUqX', artifact='fiffa')], 'relevant_definitions': 'pera'}\n",
      "--\n",
      "{'messages': [HumanMessage(content=\"what's 2+2?\", additional_kwargs={}, response_metadata={}, id='c33eae05-9717-499a-bad8-477320365cfa'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'function': {'arguments': '{\"state\":{\"messages\":[{\"content\":\"2 + 2\",\"type\":\"user\"}]}}', 'name': 'middle'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-b3fdbd18-d6d9-4fa6-b297-dcc0f54ed8c2-0', tool_calls=[{'name': 'middle', 'args': {'state': {'messages': [{'content': '2 + 2', 'type': 'user'}]}}, 'id': 'call_FZ5DM44RnGfHYUyrp0ejcUqX', 'type': 'tool_call'}])], 'relevant_definitions': 'pera'}\n",
      "--\n",
      "{'messages': [HumanMessage(content=\"what's 2+2?\", additional_kwargs={}, response_metadata={}, id='c33eae05-9717-499a-bad8-477320365cfa')], 'relevant_definitions': 'pera'}\n",
      "--\n",
      "{'messages': []}\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "all_states = []\n",
    "for state in definitions_agent.get_state_history(config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "):\n",
    "    print(state.values)\n",
    "    all_states.append(state)\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from LegalDefAgent.src.utils import draw_graph\n",
    "\n",
    "\n",
    "draw_graph(definitions_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, Literal, Sequence, Dict, Any\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "@tool\n",
    "def search(state: AgentState, tool_call_id: Annotated[str, InjectedToolCallId]):\n",
    "    \"\"\"\n",
    "    Search for the definition.\n",
    "    \"\"\"\n",
    "    print(state)\n",
    "    return Command(\n",
    "        update={\"meaning_of_fun\": \"fishing\",\n",
    "                \"messages\": [ToolMessage(\"Successfully searched for the meaning of fun\", tool_call_id=tool_call_id)]\n",
    "                },\n",
    "        goto=\"answer\"\n",
    "    )\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "tools = [search]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "def answer(state: AgentState):\n",
    "    return {\"messages\": [AIMessage(content=f\"The meaning of life is one {state[\"meaning_of_fun\"]}\")]}\n",
    "    \n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    meaning_of_fun: str\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"answer\"\n",
    "\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"answer\", answer)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "#workflow.add_edge(\"agent\", \"tools\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's the definition of fun?\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search (call_ZFu09f7ytVMWwhzdor9kcjFn)\n",
      " Call ID: call_ZFu09f7ytVMWwhzdor9kcjFn\n",
      "  Args:\n",
      "    state: {'messages': [{'content': \"what's the definition of fun?\", 'type': 'user'}], 'meaning_of_fun': 'definition'}\n",
      "None\n",
      "\n",
      "\n",
      "{'messages': [BaseMessage(content=\"what's the definition of fun?\", additional_kwargs={}, response_metadata={}, type='user')], 'meaning_of_fun': 'definition'}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search\n",
      "\n",
      "Successfully searched for the meaning of fun\n",
      "None\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The meaning of life is one fishing\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in app.stream(\n",
    "    {\"messages\": [(\"user\", \"what's the definition of fun?\")]},\n",
    "    {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk['messages'][-1].pretty_print())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"what's the definition of fun?\", additional_kwargs={}, response_metadata={}, id='558d8475-959c-41e0-a0c6-51b1fc0b20ee'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IZsGTnuuXjKMengQSUgZMuJH', 'function': {'arguments': '{\"state\":{\"messages\":[{\"content\":\"what\\'s the definition of fun?\",\"type\":\"user\"}]}}', 'name': 'search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 110, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c1f65fe9-a536-4a0e-8c59-bbd851a11834-0', tool_calls=[{'name': 'search', 'args': {'state': {'messages': [{'content': \"what's the definition of fun?\", 'type': 'user'}]}}, 'id': 'call_IZsGTnuuXjKMengQSUgZMuJH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 110, 'output_tokens': 31, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully searched for the meaning of life', name='search', id='cb40ebf4-6627-47ab-adbb-312fb328864a', tool_call_id='call_IZsGTnuuXjKMengQSUgZMuJH')], 'meaning_of_life': 'fishing'}\n",
      "{'messages': [HumanMessage(content=\"what's the definition of fun?\", additional_kwargs={}, response_metadata={}, id='558d8475-959c-41e0-a0c6-51b1fc0b20ee'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IZsGTnuuXjKMengQSUgZMuJH', 'function': {'arguments': '{\"state\":{\"messages\":[{\"content\":\"what\\'s the definition of fun?\",\"type\":\"user\"}]}}', 'name': 'search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 110, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c1f65fe9-a536-4a0e-8c59-bbd851a11834-0', tool_calls=[{'name': 'search', 'args': {'state': {'messages': [{'content': \"what's the definition of fun?\", 'type': 'user'}]}}, 'id': 'call_IZsGTnuuXjKMengQSUgZMuJH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 110, 'output_tokens': 31, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [HumanMessage(content=\"what's the definition of fun?\", additional_kwargs={}, response_metadata={}, id='558d8475-959c-41e0-a0c6-51b1fc0b20ee')]}\n",
      "{'messages': []}\n"
     ]
    }
   ],
   "source": [
    "all_states = []\n",
    "for state in app.get_state_history(config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "):\n",
    "    all_states.append(state)\n",
    "\n",
    "for state in all_states:\n",
    "    print(state.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
