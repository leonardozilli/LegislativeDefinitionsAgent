{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Definition retrieval and generation agent with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \\\n",
    "    datasets==2.19.1 \\\n",
    "    langchain-pinecone==0.1.1 \\\n",
    "    langchain-openai==0.1.9 \\\n",
    "    langchain==0.2.5 \\\n",
    "    langchain-core==0.2.9 \\\n",
    "    langgraph==0.1.1 \\\n",
    "    semantic-router==0.0.48 \\\n",
    "    serpapi==0.1.5 \\\n",
    "    google-search-results==2.4.2 \\\n",
    "    pygraphviz==1.12  # for visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Agent Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research agent will consist of a function calling AI agent that has access to several tools that it can use to find information on a particular topic. \n",
    "It will be able to use several tools over multiple steps, meaning it can find information on one topic, broaden the scope of knowledge on this topic and _even_ investigate parallel topics where relevant.\n",
    "\n",
    "The tools we will be using are:\n",
    "\n",
    "* **RAG search**: We will create a vector knowledge base containing the embeddings of documents from the datasets. This tool provides our agent with access to this knowledge. [*pinecone*, *faiss*]\n",
    "* **RAG search with filter**: The agent may need more information from a specific document, this tool allows our agent to do just that. [*opensearch*]\n",
    "* **Reference resolution**: If a definition contains a reference, this tool enable the agent to resolve it.\n",
    "* **Web search**: This tool provides our agent with access to online thesaurus and standardized vocabularies (e.g. https://op.europa.eu/it/web/eu-vocabularies)\n",
    "* **Final answer**: We create a custom final answer tool that forces our agent to output information in a specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph State\n",
    "\n",
    "We will define a custom graph state to support our agent-oriented decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four parts to our agent state, those are:\n",
    "\n",
    "* `input`: this is the user's most recent query, usually this would be a question that we want to answer with our research agent.\n",
    "* `chat_history`: we are building a conversational agent that can support multiple interactions, to allow previous interactions to provide additional context throughout our agent logic we include the chat history in the agent state.\n",
    "* `intermediate_steps`: provides a record of all steps the research agent will take between the user asking a question via `input` and the agent providing a final answer. This can include things like \"search arxiv\", \"perform general purpose web search\", etc. These intermediate steps are crucial to allowing the agent to follow a path of coherent actions and ultimately producing an informed final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two RAG-focused tools for our agent. The `rag_search` allows the agent to perform a simple RAG search for some information across _all_ indexed documents, retrieving the most semantically similar documents based on a similarity score.\n",
    "\n",
    "The `rag_opensearch` searches instead _within_ the documents in the dataset for a specific string contained within the document. This allows the agent to find specific information within a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Knowledge Base Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the documents into a vector space using a pre-trained model. This base will be used to find the most similar documents to a given query.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Which documents do we encode? The whole dataset? Only the documents with definitions?\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "What do we encode? The whole documents? The keywords? The definitions?\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "How do we encode the documents? What model do we use?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Controller** is our graph's decision maker. It decides which path we should take down our graph. It functions similarly to an agent but is much simpler and reliable.\n",
    "\n",
    "The Controller consists of an LLM provided with a set of potential function calls (i.e. our tools) that it can decide to use â€” we force it to use _at least_ one of those tool using the `tool_choice=\"any\"` setting (see below). Our Controller only makes the decision to use a tool, it doesn't execute the tool code itself (we do that seperately in our graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt for the Oracle will emphasize it's decision making ability within the `system_prompt`, leave a placeholder for us to later insert `chat_history`, and provide a place for us to insert the user `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
    "Given the user's query you must decide what to do with it based on the\n",
    "list of tools provided to you.\n",
    "\n",
    "If you see that a tool has been used (in the scratchpad) with a particular\n",
    "query, do NOT use that same tool with the same query again. Also, do NOT use\n",
    "any tool more than twice (ie, if the tool appears in the scratchpad twice, do\n",
    "not use it again).\n",
    "\n",
    "You should aim to collect information from a diverse range of sources before\n",
    "providing the answer to the user. Once you have collected plenty of information\n",
    "to answer the user's question (stored in the scratchpad) use the final_answer\n",
    "tool.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"assistant\", \"scratchpad: {scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must initialize our `llm` (for this we use `gpt-4o`) and then create the _runnable_ pipeline of our Oracle.\n",
    "\n",
    "The runnable connects our inputs (the user `input` and `chat_history`) to our `prompt`, and our `prompt` to our `llm`. It is also where we _bind_ our tools to the LLM and enforce function calling via `tool_choice=\"any\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolCall, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tools=[\n",
    "    rag_search_filter,\n",
    "    rag_search,\n",
    "    fetch_arxiv,\n",
    "    web_search,\n",
    "    final_answer\n",
    "]\n",
    "\n",
    "# define a function to transform intermediate_steps from list\n",
    "# of AgentAction to scratchpad string\n",
    "def create_scratchpad(intermediate_steps: list[AgentAction]):\n",
    "    research_steps = []\n",
    "    for i, action in enumerate(intermediate_steps):\n",
    "        if action.log != \"TBD\":\n",
    "            # this was the ToolExecution\n",
    "            research_steps.append(\n",
    "                f\"Tool: {action.tool}, input: {action.tool_input}\\n\"\n",
    "                f\"Output: {action.log}\"\n",
    "            )\n",
    "    return \"\\n---\\n\".join(research_steps)\n",
    "\n",
    "oracle = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"scratchpad\": lambda x: create_scratchpad(\n",
    "            intermediate_steps=x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
